{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL - ass2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dTD0EP25KB4E",
        "LPecNrrn5N_i"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTD0EP25KB4E"
      },
      "source": [
        "# Load Files\n",
        "Loading:\n",
        "*   pairsDevTest.txt\n",
        "*   pairsDevTrain.txt\n",
        "*   lfwa.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHQApDaEKhvR"
      },
      "source": [
        "!unzip 'lfwa.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPecNrrn5N_i"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNMl-Bn65Pvj"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense, Conv2D,Lambda, Flatten, MaxPooling2D, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Nadam\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "\n",
        "import gc\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "import google.colab.patches as colab_cv2\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R8JfNp0tvbE"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H64A3vrtuh-"
      },
      "source": [
        "def load_data(train_path, test_path, image_size, image_path):\n",
        "    \"\"\"\n",
        "      Description: loads the data, splits it to labels and features \n",
        "\n",
        "      Input: \n",
        "        train_path - the path to the train dataset \n",
        "        test_path - the path to the test dataset\n",
        "        image_path - path to image folders\n",
        "        image_size - the image size \n",
        "\n",
        "      Output:\n",
        "        train, test, train_y, test_y - numpy arrays of the train and test datasets, including the images\n",
        "\n",
        "    \"\"\"\n",
        "    train_equal, train_not_equal = load_files(train_path, image_path, image_size)\n",
        "    test_equal, test_not_equal = load_files(test_path, image_path, image_size)\n",
        "\n",
        "    train_y = np.ones(len(train_equal))\n",
        "    train_y_not_equal = np.zeros(len(train_not_equal))\n",
        "    train_y = np.concatenate((train_y, train_y_not_equal), axis=0)\n",
        "    train = np.concatenate((train_equal, train_not_equal), axis=0)\n",
        "    indices = np.arange(train_y.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    train = train[indices]\n",
        "    train_y = train_y[indices]\n",
        "\n",
        "    test_y = np.ones(len(test_equal))\n",
        "    test_y_not_equal = np.zeros(len(test_not_equal))\n",
        "    test_y = np.concatenate((test_y, test_y_not_equal), axis=0)\n",
        "    test = np.concatenate((test_equal, test_not_equal), axis=0)\n",
        "    indices = np.arange(test_y.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    test = test[indices]\n",
        "    test_y = test_y[indices]   \n",
        "    \n",
        "    train = [train[:, 1, :, :, :], train[:, 0, :, :, :]]\n",
        "    test = [test[:, 1, :, :, :], test[:, 0, :, :, :]]\n",
        "\n",
        "    return train, test, train_y, test_y\n",
        "\n",
        "def load_images(sample, equal, image_path, image_size):\n",
        "    \"\"\"\n",
        "      Description: loads pairs of images (equal or not)\n",
        "\n",
        "      Input: \n",
        "        sample - string of pairs to load\n",
        "        equal - indication on whether the pair of persons in the sample are equal or not\n",
        "        image_path - path to image folders\n",
        "        image_size - the image size \n",
        "\n",
        "      Output:\n",
        "        first_image - the image of the first person \n",
        "        second_image - the image of the second person\n",
        "\n",
        "    \"\"\"\n",
        "    sample = sample.split(\"\\t\")\n",
        "    first_person_name = sample[0]\n",
        "    first_image_name = str(int(sample[1])).zfill(4)\n",
        "\n",
        "    if equal:\n",
        "        second_person_name = first_person_name\n",
        "        second_image_name = str(int(sample[2])).zfill(4)\n",
        "    else:\n",
        "        second_person_name = sample[2]\n",
        "        second_image_name = str(int(sample[3])).zfill(4)\n",
        "\n",
        "    first_image_path = image_path + first_person_name + \"/\" + first_person_name + \"_\" + first_image_name + \".jpg\"\n",
        "    first_image = cv2.imread(first_image_path, 0)\n",
        "    first_image = cv2.resize(first_image, (image_size, image_size))\n",
        "    first_image = np.array(first_image).reshape(image_size, image_size, 1)\n",
        "    second_image_path = image_path + second_person_name + \"/\" + second_person_name + \"_\" + second_image_name + \".jpg\"\n",
        "    second_image = cv2.imread(second_image_path, 0)\n",
        "    second_image = cv2.resize(second_image, (image_size, image_size))\n",
        "    second_image = np.array(second_image).reshape(image_size, image_size, 1)\n",
        "\n",
        "    return [first_image, second_image]\n",
        "\n",
        "\n",
        "def load_files(file_path, image_path, image_size):\n",
        "    \"\"\"\n",
        "      Description: splits the text file into equal and non equal lists of pairs\n",
        "\n",
        "      Input: \n",
        "        file_path - path to the samples data (train or test)\n",
        "        image_path - path to image folders\n",
        "        image_size - the image size \n",
        "\n",
        "      Output:\n",
        "        data_equals - list of all equal pairs\n",
        "        data_not_equals - list of all non equal pairs\n",
        "\n",
        "    \"\"\"\n",
        "    data_equals = []\n",
        "    data_not_equals = []\n",
        "    i = 0\n",
        "    with open(file_path) as fp:\n",
        "        line = fp.readline()\n",
        "        amount_of_samples = int(line) * 2\n",
        "        while i < amount_of_samples:\n",
        "            i_sample = fp.readline()\n",
        "            if i < amount_of_samples / 2:\n",
        "                data_equals.append(load_images(i_sample, True, image_path, image_size))\n",
        "            else:\n",
        "                data_not_equals.append(load_images(i_sample, False, image_path, image_size))\n",
        "            i += 1\n",
        "    return data_equals, data_not_equals\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqvbvR81tm-k"
      },
      "source": [
        "# Siamese model (according to the architecture in the paper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H3DRykTp-Sc"
      },
      "source": [
        "def create_model_with_batchnorm(image_shape, optimizer):\n",
        "    \"\"\"\n",
        "      Description: creates the Siamese network with batchnorm layers - between convolution layers and non liniearites activation layers\n",
        "\n",
        "      Input: \n",
        "        impage_shape - the images shape \n",
        "        optimizer - the optimizer to use in the training \n",
        "\n",
        "      Output:\n",
        "        siamese_net - the network model\n",
        "\n",
        "    \"\"\"\n",
        "    w_init = initializers.RandomNormal(mean=0.0, stddev=0.01)\n",
        "    b_init = initializers.RandomNormal(mean=0.5, stddev=0.01)\n",
        "    first_input = Input(image_shape)\n",
        "    second_input = Input(image_shape)\n",
        "    model = Sequential()\n",
        "\n",
        "    # input layer - first convolution\n",
        "    model.add(Conv2D(64, (10,10), input_shape=image_shape,\n",
        "                      kernel_initializer=w_init, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "    # second convolution layer\n",
        "    model.add(Conv2D(128, (7,7), input_shape=image_shape,\n",
        "                    bias_initializer=b_init,  kernel_initializer=w_init, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "    # third convolution layer\n",
        "    model.add(Conv2D(128, (4,4), input_shape=image_shape,\n",
        "                    bias_initializer=b_init,  kernel_initializer=w_init, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "    # fourth convolution layer\n",
        "    model.add(Conv2D(256, (4,4), input_shape=image_shape,\n",
        "                bias_initializer=b_init,  kernel_initializer=w_init, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    # fully connected layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='sigmoid',\n",
        "                  kernel_regularizer=l2(2e-4)\n",
        "            , bias_initializer=b_init,  kernel_initializer=w_init))\n",
        "\n",
        "    encoded_f = model(first_input)\n",
        "    encoded_s = model(second_input)\n",
        "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "    L1_distance = L1_layer([encoded_f, encoded_s])\n",
        "    prediction = Dense(1,activation='sigmoid',bias_initializer=b_init, kernel_initializer=w_init)(L1_distance)\n",
        "    siamese_net = Model(inputs=[first_input,second_input],outputs=prediction)\n",
        "\n",
        "    siamese_net.compile(loss='binary_crossentropy', metrics=['binary_accuracy'],\n",
        "                      optimizer=optimizer)\n",
        "    return siamese_net"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9J2BG6UtfW_"
      },
      "source": [
        "def create_model(image_shape, optimizer):\n",
        "    \"\"\"\n",
        "      Description: creates the Siamese network according to the architecture in the paper\n",
        "\n",
        "      Input: \n",
        "        impage_shape - the images shape \n",
        "        optimizer - the optimizer to use in the training \n",
        "\n",
        "      Output:\n",
        "        siamese_net - the network model\n",
        "\n",
        "    \"\"\"\n",
        "    w_init = initializers.RandomNormal(mean=0.0, stddev=0.01)\n",
        "    b_init = initializers.RandomNormal(mean=0.5, stddev=0.01)\n",
        "    first_input = Input(image_shape)\n",
        "    second_input = Input(image_shape)\n",
        "    model = Sequential()\n",
        "\n",
        "    # input layer - first convolution\n",
        "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=image_shape,\n",
        "                      kernel_initializer=w_init, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "    # second convolution layer\n",
        "    model.add(Conv2D(128, (7,7), activation='relu', input_shape=image_shape,\n",
        "                    bias_initializer=b_init,  kernel_initializer=w_init, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "    # third convolution layer\n",
        "    model.add(Conv2D(128, (4,4), activation='relu', input_shape=image_shape,\n",
        "                    bias_initializer=b_init,  kernel_initializer=w_init, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "    # fourth convolution layer\n",
        "    model.add(Conv2D(256, (4,4), activation='relu', input_shape=image_shape,\n",
        "                bias_initializer=b_init,  kernel_initializer=w_init, kernel_regularizer=l2(2e-4)))\n",
        "\n",
        "    # fully connected layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='sigmoid',kernel_regularizer=l2(2e-4),\n",
        "                    bias_initializer=b_init,  kernel_initializer=w_init))\n",
        "\n",
        "    encoded_f = model(first_input)\n",
        "    encoded_s = model(second_input)\n",
        "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "    L1_distance = L1_layer([encoded_f, encoded_s])\n",
        "    prediction = Dense(1,activation='sigmoid',bias_initializer=b_init, kernel_initializer=w_init)(L1_distance)\n",
        "    siamese_net = Model(inputs=[first_input,second_input],outputs=prediction)\n",
        "\n",
        "    siamese_net.compile(loss='binary_crossentropy', metrics=['binary_accuracy'],\n",
        "                      optimizer=optimizer)\n",
        "    return siamese_net"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFuEtYYfEsBZ"
      },
      "source": [
        "import datetime\n",
        "\n",
        "def train_model(train, test, y_train, y_test, image_shape, learning_rate, optimizer_name, max_epochs, batch_size, use_batchnorm):\n",
        "    \"\"\"\n",
        "      Description: training the Siamese network\n",
        "\n",
        "      Input: \n",
        "        train - train dataset\n",
        "        test -  test dataset\n",
        "        y_train -  train labels\n",
        "        y_test - test labels\n",
        "        image_shape - the images shape\n",
        "        learning_rate - the learning rate \n",
        "        optimizer_name - the optimizer to use for the model training\n",
        "        max_epochs -  the maximum amount of ephocs\n",
        "        batch_size - the batch size for the input in the network\n",
        "        use_batchnorm - boolean variable, if the CNN network will contain batch normalization layers\n",
        "      Output:\n",
        "        history - the history of the trained model along the training process\n",
        "        model - the trained model \n",
        "\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    optimizer = get_optimizer(optimizer_name, learning_rate)\n",
        "    model = create_model(image_shape, optimizer)\n",
        "    if use_batchnorm:\n",
        "      model = create_model_with_batchnorm(image_shape, optimizer) \n",
        "\n",
        "    # Define Tensorboard as a Keras callback\n",
        "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    tensorboard = TensorBoard(\n",
        "      log_dir= logdir,\n",
        "      histogram_freq=1)      \n",
        "    callbacks = [\n",
        "        tensorboard,\n",
        "        EarlyStopping(monitor='val_loss',\n",
        "                      min_delta=0,\n",
        "                      patience=0,\n",
        "                      verbose=0, mode='auto'), ]\n",
        "    history = model.fit(train, y_train, epochs=max_epochs, batch_size=batch_size, validation_split=0.1, verbose=1,\n",
        "                        callbacks=callbacks)\n",
        "    loss, acc = model.evaluate(test, y_test, verbose=0)\n",
        "    history = history.history\n",
        "    history[\"test_binary_accuracy\"] = acc\n",
        "    history[\"test_loss\"] = loss\n",
        "    end = time.time()\n",
        "    print(end - start)\n",
        "    history[\"time\"] = end - start\n",
        "\n",
        "    return history, model"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fsyZ5YjtjDy"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo6d3ef2tLHv"
      },
      "source": [
        "def get_optimizer(optimizer_name, learning_rate):\n",
        "  \"\"\"\n",
        "    Description: this is an help function that returns the optimizer object\n",
        "\n",
        "    Input:\n",
        "      optimizer_name - the optimizer name \n",
        "      learning_rate - the learning rate for the optimizer init\n",
        "\n",
        "    Output: \n",
        "      The optimizer object\n",
        "  \"\"\" \n",
        "  if optimizer_name == 'adam':\n",
        "      return Adam(lr=learning_rate)\n",
        "  if optimizer_name == \"nadam\":\n",
        "      return Nadam(lr=learning_rate)\n",
        "  else:\n",
        "      return SGD(lr=learning_rate, momentum=0.9)\n",
        "\n",
        "\n",
        "def create_accuracy_vs_epoch_plot(learning_rates_history, learning_rates):\n",
        "  \"\"\"\n",
        "    Description: plots the graphs of the accuracy vs epochs values\n",
        "\n",
        "    Input:\n",
        "      learning_rates_history - the history of loss values of the model along the training process \n",
        "      learning_rate - the list of examined learning rates\n",
        "\n",
        "    Output: \n",
        "      the plots\n",
        "  \"\"\"\n",
        "  for learning_rate in learning_rates:\n",
        "    fig, axs = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(20, 5))\n",
        "    j = -1\n",
        "    optimizer_history = learning_rates_history[learning_rate]\n",
        "    for optimizer in optimizer_history:\n",
        "        if j == 2:\n",
        "            j = 0\n",
        "        else:\n",
        "            j += 1\n",
        "        axs[j].plot(optimizer_history[optimizer]['history']['binary_accuracy'])\n",
        "        axs[j].plot(optimizer_history[optimizer]['history']['val_binary_accuracy'])\n",
        "        axs[j].set_title(optimizer)\n",
        "        axs[j].legend(['train', 'validation'], loc='upper right')\n",
        "        axs[j].set(ylabel='accuracy', xlabel='epoch')\n",
        "\n",
        "    title = 'accuracy vs epoch, learning rate =' + str(learning_rate)\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "    plt.savefig(f'accuracy_vs_epoch_plot_lr_{learning_rate}.png')\n",
        "\n",
        "\n",
        "def create_loss_vs_epoch_plot(learning_rates_history, learning_rates):\n",
        "  \"\"\"\n",
        "    Description: plots the graphs of the loss vs epochs values\n",
        "\n",
        "    Input:\n",
        "      learning_rates_history - the history of loss values of the model along the training process \n",
        "      learning_rate - the list of examined learning rates\n",
        "\n",
        "    Output: \n",
        "      the plots\n",
        "  \"\"\"\n",
        "  for learning_rate in learning_rates:\n",
        "    fig, axs = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(20, 5))\n",
        "    j = -1\n",
        "    optimizer_history = learning_rates_history[learning_rate]\n",
        "    for optimizer in optimizer_history:\n",
        "        if j == 2:\n",
        "            j = 0\n",
        "        else:\n",
        "            j += 1\n",
        "        axs[j].plot(optimizer_history[optimizer]['history']['loss'])\n",
        "        axs[j].plot(optimizer_history[optimizer]['history']['val_loss'])\n",
        "        axs[j].set_title(optimizer)\n",
        "        axs[j].legend(['train', 'validation'], loc='upper right')\n",
        "        axs[j].set(ylabel='loss', xlabel='epoch')\n",
        "\n",
        "    title = 'loss vs epoch, learning rate =' + str(learning_rate)\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "    plt.savefig(f'loss_vs_epoch_plot_lr_{learning_rate}.png')\n",
        "\n",
        "def print_results_summary(learning_rates_history, learning_rates):\n",
        "  \"\"\"\n",
        "    Description: prints a summary of the results for all the models\n",
        "\n",
        "    Input:\n",
        "      history - the history of the model results along the training process\n",
        "      learning_rates - the list of examined learning rates\n",
        "\n",
        "    Output: \n",
        "      a summary of the results\n",
        "  \"\"\"\n",
        "  for learning_rate in learning_rates:\n",
        "      optimizer_history = learning_rates_history[learning_rate]\n",
        "      for optimizer in optimizer_history:\n",
        "          history = optimizer_history[optimizer]['history']\n",
        "\n",
        "          results = \"\"\n",
        "          results += 'learning rate: ' + str(learning_rate) + \",\" \n",
        "          results += \"optimizer: \" + str(optimizer).upper() + \",\"\n",
        "          results += 'Epochs to early stop: ' + str(len(history['loss'])) + \",\"\n",
        "          results += 'time: ' + str(history['time']) + \",\"\n",
        "          results += 'train_loss: ' + str(history['loss'][-1]) + \",\"\n",
        "          results += 'train_acc: ' + str(history['binary_accuracy'][-1]) + \",\"\n",
        "          results += 'val_loss: ' + str(history['val_loss'][-1]) + \",\"\n",
        "          results += 'val_acc: ' + str(history['val_binary_accuracy'][-1]) + \",\"\n",
        "          results += 'test_loss: ' + str(history['test_loss']) + \",\"\n",
        "          results += 'test_acc: ' + str(history['test_binary_accuracy'])\n",
        "\n",
        "          print(results)\n",
        "\n",
        "def create_classifications_examples(image_shape, test, y_test, model):\n",
        "  \"\"\"\n",
        "    Description: prints examples of misclassifications and right classifications\n",
        "\n",
        "    Input:\n",
        "      image_shape - the images size \n",
        "      test - the test dataset\n",
        "      y_test -  the testset labels\n",
        "      model - the model in which the function will search for the examples\n",
        "\n",
        "    Output: \n",
        "      images of misclassifications and right classifications\n",
        "  \"\"\"\n",
        "  y_tag = model.predict(test)\n",
        "  y_tag[y_tag < 0.5] = 0\n",
        "  y_tag[y_tag >= 0.5] = 1\n",
        "\n",
        "  predicted_N_wrong = []\n",
        "  predicted_Y_wrong = []\n",
        "  predicted_N_right = []\n",
        "  predicted_Y_right = []\n",
        "\n",
        "  for i in range(len(y_tag)):\n",
        "      y_predict = y_tag[i][0]\n",
        "      true_label = y_test[i]\n",
        "\n",
        "      if y_predict < true_label:\n",
        "          predicted_N_wrong.append(i)\n",
        "      elif y_predict > true_label:\n",
        "          predicted_Y_wrong.append(i)\n",
        "      elif y_predict == 1:\n",
        "          predicted_Y_right.append(i)\n",
        "      elif y_predict == 0:\n",
        "          predicted_N_right.append(i)\n",
        "\n",
        "  print(\"Predicted not equals but equals\")\n",
        "  print(len(predicted_N_wrong))\n",
        "  print(\"Predicted equals but not equals\")\n",
        "  print(len(predicted_Y_wrong))\n",
        "  print(\"Predicted equals and equals\")\n",
        "  print(len(predicted_Y_right))\n",
        "  print(\"Predicted not equals and not equals\")\n",
        "  print(len(predicted_N_right))\n",
        "\n",
        "  if (len(predicted_N_wrong) > 0):\n",
        "      print(\"Misclassification - predicted not equals but equals\")\n",
        "      colab_cv2.cv2_imshow(test[0][predicted_N_wrong[len(predicted_N_wrong) // 2]])\n",
        "      colab_cv2.cv2_imshow(test[1][predicted_N_wrong[len(predicted_N_wrong) // 2]])\n",
        "  if (len(predicted_Y_wrong) > 0):\n",
        "      print(\"Misclassification - predicted equals but not equals\")\n",
        "      colab_cv2.cv2_imshow(test[0][predicted_Y_wrong[len(predicted_Y_wrong) // 2]])\n",
        "      colab_cv2.cv2_imshow(test[1][predicted_Y_wrong[len(predicted_Y_wrong) // 2]])\n",
        "\n",
        "  if (len(predicted_N_right) > 0):\n",
        "      print(\"Right classification - predicted not equals and not equals\")\n",
        "      colab_cv2.cv2_imshow(test[0][predicted_N_right[len(predicted_N_right) // 2]])\n",
        "      colab_cv2.cv2_imshow(test[1][predicted_N_right[len(predicted_N_right) // 2]])\n",
        "\n",
        "  if (len(predicted_Y_right) > 0):\n",
        "      print(\"Right classification - predicted equals and equals\")\n",
        "      colab_cv2.cv2_imshow(test[0][predicted_Y_right[len(predicted_Y_right) // 2]])\n",
        "      colab_cv2.cv2_imshow(test[1][predicted_Y_right[len(predicted_Y_right) // 2]])\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuRvawIBvOSD"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOpOJM69u6za"
      },
      "source": [
        "from collections import defaultdict\n",
        "all_models_histories= defaultdict(lambda: defaultdict(dict))\n",
        "all_models = defaultdict(lambda: defaultdict(dict))\n",
        "learning_rates = [0.0005, 5e-05, 5e-06]\n",
        "optimizers = [\"nadam\", \"adam\"]\n",
        "\n",
        "image_size = 105\n",
        "shape = (image_size, image_size, 1)\n",
        "epochs_size = 100\n",
        "batch_s = 100\n",
        "dropout = 0.3\n",
        "\n",
        "test_path=\"pairsDevTest.txt\" \n",
        "train_path=\"pairsDevTrain.txt\" \n",
        "image_path=\"lfw2/lfw2/\"\n",
        "verbose = 0\n",
        "use_batchnorm = True\n",
        "\n",
        "train, test, y_train, y_test = load_data(train_path, test_path, image_size, image_path)\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "  for optimizer in optimizers:\n",
        "    history, model = train_model(train, test, y_train, y_test, shape, learning_rate, optimizer, epochs_size, batch_s, use_batchnorm)\n",
        "    all_models_histories[learning_rate][optimizer]['history'] = history\n",
        "    all_models[learning_rate][optimizer]['model'] = model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyVEmZTIQygh"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AJGVe2rQ8D9"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABR_nfBtFSr3"
      },
      "source": [
        "# Report and visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij_gICOpxGF5"
      },
      "source": [
        "print_results_summary(all_models_histories, learning_rates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnaMbhrbRMeB"
      },
      "source": [
        "create_loss_vs_epoch_plot(all_models_histories, learning_rates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rQfFAapSPaX"
      },
      "source": [
        "create_accuracy_vs_epoch_plot(all_models_histories, learning_rates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVMWHfY4dO6x"
      },
      "source": [
        "# Classification examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkdMlnG6dQux"
      },
      "source": [
        "best_model = all_models[0.0005]['nadam']['model']\n",
        "create_classifications_examples(shape, test, y_test, best_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMKGDc-Es7_G"
      },
      "source": [
        "# Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lIQ4Knbs7US"
      },
      "source": [
        "print(f'Number of pairs in train: {len(y_train)}')\n",
        "print(f'Number of equal pairs in train: {len(y_train[y_train==1])}')\n",
        "print(f'Number of not equal pairs in train: {len(y_train[y_train==0])}')\n",
        "\n",
        "print(f'Number of pairs in test: {len(y_test)}')\n",
        "print(f'Number of equal pairs in train: {len(y_test[y_test==1])}')\n",
        "print(f'Number of not equal pairs in train: {len(y_test[y_test==0])}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}